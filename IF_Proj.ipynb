{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "806245d5a40843d79de75aa41e6b33a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_56aa0e4ebef04967b931fff799deb609",
              "IPY_MODEL_f61284fce6fb4395b89553b7b2e77cf5",
              "IPY_MODEL_b8eef09c18b64eb98bdb101888dd2a85"
            ],
            "layout": "IPY_MODEL_bf965112b4b4410ba49c9d24e4fa08bf"
          }
        },
        "56aa0e4ebef04967b931fff799deb609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d76c4e6b61cf41e4a3deb816ae35daed",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2e0933e9de10470188679cefa9585f34",
            "value": "Unsloth:â€‡Tokenizingâ€‡[&quot;text&quot;]â€‡(num_proc=16):â€‡100%"
          }
        },
        "f61284fce6fb4395b89553b7b2e77cf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca96ff189ca4bdc8db84bb86b62a5f1",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe7e350f493344c989f7dcd8a06f9871",
            "value": 500
          }
        },
        "b8eef09c18b64eb98bdb101888dd2a85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_683ba52a06a64918bbb2c6d255fe0a45",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_efd97cf50a5e4dcba1ba85e36251d5fd",
            "value": "â€‡500/500â€‡[00:01&lt;00:00,â€‡642.32â€‡examples/s]"
          }
        },
        "bf965112b4b4410ba49c9d24e4fa08bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76c4e6b61cf41e4a3deb816ae35daed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e0933e9de10470188679cefa9585f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ca96ff189ca4bdc8db84bb86b62a5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe7e350f493344c989f7dcd8a06f9871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "683ba52a06a64918bbb2c6d255fe0a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd97cf50a5e4dcba1ba85e36251d5fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation\n"
      ],
      "metadata": {
        "id": "65dfIrGhMIav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os, re, random\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.55.4\n",
        "!pip install --no-deps trl==0.22.2"
      ],
      "metadata": {
        "id": "PO92YCVXBiUp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n"
      ],
      "metadata": {
        "id": "D9tOpB3AMPfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"INK-USC/riddle_sense\", split=\"train\", trust_remote_code=True)\n",
        "ansKey = {'A':0,'B':1,'C':2,'D':3,'E':4}"
      ],
      "metadata": {
        "id": "WRByWGDNqYXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3965593-3242-4c97-e1e1-dbf98543921e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['question'][0])\n",
        "print(dataset['answerKey'][0])\n",
        "print(dataset['choices'][0]['text'][4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUE7eQx7rweA",
        "outputId": "d8102213-013e-40e2-87b2-dadc4f8a0b50"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man is incarcerated in prison, and as his punishment he has to carry a one tonne bag of sand backwards and forwards across a field the size of a football pitch.  What is the one thing he can put in it to make it lighter?\n",
            "E\n",
            "hole\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getExample():\n",
        "  rndm = random.randint(501, 3510)\n",
        "  shot = \"Riddle: \" + dataset['question'][rndm] + \"\\n\"\n",
        "  ans = dataset['answerKey'][rndm]\n",
        "  shot += \"Answer: \" + dataset['choices'][rndm]['text'][ansKey[ans]]\n",
        "  return shot\n",
        "\n",
        "print(getExample())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsq5Eo8VtHXJ",
        "outputId": "16bd49f6-60ca-48af-f4a2-54e921de0fe2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riddle: 1 man runs 3 blocks and then comes home and there is 2 maced men who r they?\n",
            "Answer: catcher and umpier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "r1kO6265MQu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code from https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_v0.3_(7B)-Conversational.ipynb\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/mistral-7b-v0.3\",\n",
        "    max_seq_length = 4096, # Choose any!\n",
        "    dtype = None, # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "    load_in_4bit = True, # Use 4bit quantization to reduce memory usage. Can be False.\n",
        ")\n",
        "EOS_TOKEN = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYJxXgp7CcUz",
        "outputId": "26b89874-ae34-44cf-df90-b0d7c8266036"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Unsloth: Could not import trl.trainer.alignprop_trainer: Failed to import trl.trainer.alignprop_trainer because of the following error (look up to see its traceback):\n",
            "Failed to import trl.models.modeling_sd_base because of the following error (look up to see its traceback):\n",
            "Failed to import diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion because of the following error (look up to see its traceback):\n",
            "Failed to import diffusers.loaders.ip_adapter because of the following error (look up to see its traceback):\n",
            "JITCallable._set_src() takes 1 positional argument but 2 were given\n",
            "Unsloth: Could not import trl.trainer.ddpo_trainer: Failed to import trl.trainer.ddpo_trainer because of the following error (look up to see its traceback):\n",
            "Failed to import trl.models.modeling_sd_base because of the following error (look up to see its traceback):\n",
            "Failed to import diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion because of the following error (look up to see its traceback):\n",
            "Failed to import diffusers.loaders.ip_adapter because of the following error (look up to see its traceback):\n",
            "JITCallable._set_src() takes 1 positional argument but 2 were given\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:unsloth_zoo.log:Unsloth: Failed to import trl openenv: No module named 'trl.experimental'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.12.4: Fast Mistral patching. Transformers: 4.55.4.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"mistral\",\n",
        "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
        "    map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
        ")\n",
        "\n",
        "#FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
      ],
      "metadata": {
        "id": "pHLcx-yQHy9J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FINETUNING"
      ],
      "metadata": {
        "id": "wyfGVF8xBamm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traindataset = load_dataset(\"INK-USC/riddle_sense\", split=\"train[:500]\", trust_remote_code=True)\n",
        "traindataset = traindataset.map(lambda example, idx: {\"question\": 'Riddle: '+ example[\"question\"] + '\\nAnswer: ' + example[\"choices\"]['text'][ansKey[example['answerKey']]] + EOS_TOKEN}, with_indices=True)\n",
        "traindataset = traindataset.map(remove_columns=['choices','answerKey'])\n",
        "print(traindataset['question'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTA9kwCTkkVW",
        "outputId": "5a0db88b-b501-420a-867b-7e71e5a9d136"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riddle: A man is incarcerated in prison, and as his punishment he has to carry a one tonne bag of sand backwards and forwards across a field the size of a football pitch.  What is the one thing he can put in it to make it lighter?\n",
            "Answer: hole</s>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "\n",
        "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0.01, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dI_tGKzqLdo",
        "outputId": "97641354-2025-4fd9-bf3a-78d5a15c1190"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.01.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
            "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py:1222: UserWarning: Model has `tie_word_embeddings=True` and a tied layer is part of the adapter, but `ensure_weight_tying` is not set to True. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. Check the discussion here: https://github.com/huggingface/peft/issues/2777\n",
            "  warnings.warn(msg)\n",
            "Unsloth 2025.12.4 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
            "Unsloth: Training lm_head in mixed precision to save VRAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"question\"]\n",
        "    texts = []\n",
        "    for instruction in instructions:\n",
        "        # The 'question' field already contains the formatted riddle and answer\n",
        "        text = f\"{instruction}\"\n",
        "        texts.append(text)\n",
        "    return texts # Changed to return the list directly\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = traindataset,\n",
        "    #ataset_text_field = \"question\", # Remove this line as formatting_func will handle it\n",
        "    max_seq_length = 4096,\n",
        "    dataset_num_proc = 8,\n",
        "    formatting_func = formatting_prompts_func,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        gradient_checkpointing=True,\n",
        "\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 1,\n",
        "\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 5e-6,\n",
        "\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.00,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "806245d5a40843d79de75aa41e6b33a9",
            "56aa0e4ebef04967b931fff799deb609",
            "f61284fce6fb4395b89553b7b2e77cf5",
            "b8eef09c18b64eb98bdb101888dd2a85",
            "bf965112b4b4410ba49c9d24e4fa08bf",
            "d76c4e6b61cf41e4a3deb816ae35daed",
            "2e0933e9de10470188679cefa9585f34",
            "7ca96ff189ca4bdc8db84bb86b62a5f1",
            "fe7e350f493344c989f7dcd8a06f9871",
            "683ba52a06a64918bbb2c6d255fe0a45",
            "efd97cf50a5e4dcba1ba85e36251d5fd"
          ]
        },
        "id": "OGq7H0mUUFZ0",
        "outputId": "ddc3548a-1d43-4a8f-c8c8-f0b9db289fe6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:trl.trainer.sft_trainer:Padding-free training is enabled, but the attention implementation is not set to 'flash_attention_2'. Padding-free training flattens batches into a single sequence, and 'flash_attention_2' is the only known attention mechanism that reliably supports this. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation='flash_attention_2'` in the model configuration, or verify that your attention mechanism can handle flattened sequences.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=16):   0%|          | 0/500 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "806245d5a40843d79de75aa41e6b33a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E5lOIBbjdFrM",
        "outputId": "acc6d8a8-448c-42c4-b354-5f2bd353c1c3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 32\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 603,979,776 of 7,852,003,328 (7.69% trained)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32/32 02:24, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.374700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.816800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.959500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.277200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.639300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.206400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.283600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.086200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.035800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.256200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.123800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>2.180200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>2.298200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>2.298900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.036600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>2.220900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.452200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.554900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>2.209900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>2.294600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>2.158400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.988500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>2.192200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.382200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.934600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>2.203900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>2.778200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to prompt the model"
      ],
      "metadata": {
        "id": "AriPpOXVxQG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def promptMinstral(myprompt):\n",
        "  messages = [{\"from\":\"human\", \"value\": myprompt}]\n",
        "  inputs = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      tokenize = True,\n",
        "      add_generation_prompt = True, # Must add for generation\n",
        "      return_tensors = \"pt\",\n",
        "  ).to(\"cuda\")\n",
        "  outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True)\n",
        "  return tokenizer.batch_decode(outputs)[0]"
      ],
      "metadata": {
        "id": "qqelLuW9BtTt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regex"
      ],
      "metadata": {
        "id": "7v_1XED53AYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import math\n",
        "\n",
        "def decodeOutput(riddle):\n",
        "  decoded = re.split(r\"\\[/INST\\]\",riddle)\n",
        "  decoded = decoded[1] # cuts off prompt\n",
        "\n",
        "  riddle = re.search(\"[r|R]iddle: (.*)(<)+?\", decoded)\n",
        "  if not riddle:\n",
        "    riddlesplit = decoded.split('\\n')\n",
        "    riddle = riddlesplit[0] if len(riddlesplit[0]) > 1 else riddlesplit[1]\n",
        "    if '<' in riddle:\n",
        "      riddle = riddle.split('<')[0]\n",
        "  else:\n",
        "    riddle = riddle[1]\n",
        "\n",
        "  answer = re.search(\"[a|A]nswer: (.*)(<)+?\", decoded)\n",
        "  if not answer:\n",
        "    answer = \"None\"\n",
        "  else:\n",
        "    answer = answer[1]\n",
        "\n",
        "  if answer == \"None\" and not riddle:\n",
        "    print(decoded)\n",
        "  return riddle, answer"
      ],
      "metadata": {
        "id": "5EbwULo13Ch_"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#myInput = \"\"\"Your job is to generate a riddle given another riddle. Only send the riddle and answer. Following is an example of a riddle you should emulate. Example: \"\"\" + getExample()\n",
        "#myInput = \"\"\"You're a specialist in creating riddles. You always format the riddle following the example, putting the riddle and answer on one line each:\\\"\"\"\" + getExample() + \"\\\"\\nNow its your turn. Create a riddle.\"\n",
        "myInput = \"\"\"Your job is to generate a riddle given another riddle. Only send the riddle and answer. Following is an example of a riddle you should emulate. Example: \\n\"\"\" + getExample()\n",
        "\n",
        "rawOutput = promptMinstral(myInput)\n",
        "\n",
        "print(rawOutput)\n",
        "print(\"---------------------------\\n\")\n",
        "\n",
        "riddle, answer = decodeOutput(rawOutput)\n",
        "print(riddle, answer)"
      ],
      "metadata": {
        "id": "jZZyU4o63vZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965ce0cf-f2d4-44d2-a1ba-29e4427e0625"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] Your job is to generate a riddle given another riddle. Only send the riddle and answer. Following is an example of a riddle you should emulate. Example: \n",
            "Riddle: you ride into a town on wednesday,you stay three days and leave on wednesday.   how is that possible?\n",
            "Answer: your horses name is wednesday [/INST]: you ride into a town on wednesday,you stay three days and leave on wednesday.   how is that possible?\n",
            "Answer: your horses name is wednesday</s>\n",
            "---------------------------\n",
            "\n",
            ": you ride into a town on wednesday,you stay three days and leave on wednesday.   how is that possible? your horses name is wednesday\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Location nodes and map"
      ],
      "metadata": {
        "id": "vszVyitCP-EZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Location:\n",
        "  def __init__(self, block = False):\n",
        "    self.blocked = block\n",
        "    # room connections\n",
        "    self.north = None\n",
        "    self.east = None\n",
        "    self.west = None\n",
        "    self.south = None\n",
        "    # list of items - there's one case where there can be more than one item but its useful\n",
        "    self.items = []\n",
        "    # description of the room\n",
        "    self.desc = None\n",
        "    # bool as to whether a riddle can occur in the room\n",
        "    self.riddle = False\n",
        "    # message that the parser reads if the way is blocked.\n",
        "    self.blocked_msg = None"
      ],
      "metadata": {
        "id": "nKl9Rk91P9rd"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basicest map\n",
        "start = Location()\n",
        "hallway = Location()\n",
        "candleRoom = Location()\n",
        "bugRoom = Location(True)\n",
        "escapeRoom = Location()\n",
        "\n",
        "# Joining up the locations\n",
        "start.east = hallway\n",
        "\n",
        "hallway.east = candleRoom\n",
        "hallway.west = start\n",
        "hallway.south = bugRoom\n",
        "\n",
        "candleRoom.west = hallway\n",
        "\n",
        "bugRoom.north = hallway\n",
        "bugRoom.south = escapeRoom\n",
        "\n",
        "escapeRoom.north = bugRoom\n",
        "\n",
        "# Set items in the rooms\n",
        "candleRoom.items.append(\"candle\")\n",
        "hallway.items.append(\"rocks\")\n",
        "\n",
        "# Establishes the rooms the riddles will occur in\n",
        "start.riddle = True\n",
        "candleRoom.riddle = True\n",
        "escapeRoom.riddle = True\n",
        "\n",
        "# Set descriptions for the room\n",
        "start.desc = \"\"\"The room you first woke up in. The air is moist and the smell of mildew fills your senses. There is an opening to the east.\"\"\"\n",
        "\n",
        "hallway.desc = \"\"\"A room that connects to three others.\n",
        "It's dim but you can faintly see a light from the room to the west.\n",
        "The room to the south is loud. It sounds like something is alive down there.\n",
        "The room to the east is where you woke up.\"\"\"\n",
        "\n",
        "candleRoom.desc = \"\"\"A surprisingly bright room for the tomb. Its lined with many fires, and the walls seem gilded with gold. You're unfortunately unable to collect any.\n",
        "The room to the west is the hallway.\"\"\"\n",
        "\n",
        "bugRoom.desc = \"\"\"The sound grows louder and louder until you realize what it is. Bugs. The floor is covered in them and they all seem aggressive.\n",
        "The room to the north is the hallway.\n",
        "To the south seems to be your escape\"\"\"\n",
        "\n",
        "escapeRoom.desc = \"\"\"You can just barely see the daylight! You're nearly freed.\n",
        "'escape' to free yourself!\n",
        "\"\"\"\n",
        "\n",
        "# Set block messages to the room\n",
        "bugRoom.blocked_msg = \"The room is teeming with bugs you're unable to take so much as a step inside without them crawling on you, biting your legs up.\"\n",
        "escapeRoom.blocked_msg = \"You cannot leave. The Sphinx won't let you\""
      ],
      "metadata": {
        "id": "8pWrr8DxRn7o"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parser class"
      ],
      "metadata": {
        "id": "hL_jWcISEdwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "33PzUkp3DP-F"
      },
      "outputs": [],
      "source": [
        "class Parser:\n",
        "  def __init__(self, debug = False):\n",
        "    self.lives = 3\n",
        "    self.location = start\n",
        "    self.inventory = []\n",
        "    self.level = 0\n",
        "    self.victory = False\n",
        "    self.debug = debug\n",
        "\n",
        "  def comp(self, response, answer):\n",
        "      # answer - compares the response given to the answer the model provided.\n",
        "      #\n",
        "      # inputs: user response and the answer given by the llm\n",
        "      # returns: True or False\n",
        "      # True - the answer is correct, it passes the threshold\n",
        "      # False - the answer is incorrect.\n",
        "\n",
        "\n",
        "      # tokenize\n",
        "      resp_tokens = response.lower().split()\n",
        "      ans_tokens = answer.lower().split()\n",
        "\n",
        "      # build frequency dicts\n",
        "      resp_vec = {}\n",
        "      for w in resp_tokens:\n",
        "        resp_vec[w] = resp_vec.get(w, 0) + 1\n",
        "\n",
        "      ans_vec = {}\n",
        "      for w in ans_tokens:\n",
        "        ans_vec[w] = ans_vec.get(w, 0) + 1\n",
        "\n",
        "      # vocabulary\n",
        "      all_words = set(resp_vec.keys()) | set(ans_vec.keys())\n",
        "\n",
        "      # numeric vectors\n",
        "      v1 = [resp_vec.get(w, 0) for w in all_words]\n",
        "      v2 = [ans_vec.get(w, 0) for w in all_words]\n",
        "\n",
        "      # cosine similarity\n",
        "      dot = sum(a * b for a, b in zip(v1, v2))\n",
        "      mag1 = math.sqrt(sum(a * a for a in v1))\n",
        "      mag2 = math.sqrt(sum(b * b for b in v2))\n",
        "\n",
        "      cosine_sim = 0 if (mag1 == 0 or mag2 == 0) else dot / (mag1 * mag2)\n",
        "\n",
        "      # threshold check\n",
        "      return True if cosine_sim >= 0.4 else False\n",
        "\n",
        "      return True if eval >= 0.4 else False # thresholds the % match of the\n",
        "\n",
        "  def ask(self):\n",
        "    # promtpts the LLM to output a new riddle in the form of a json file\n",
        "    # that includes the riddle, a hint and the answer. The function runs a nested loop that\n",
        "    # checks users answer to give you multiple tries.\n",
        "    #\n",
        "    #\n",
        "    # inputs : None\n",
        "    # output : True if the question was correctly answered, False if not.\n",
        "\n",
        "    # Input prompt\n",
        "    myInput = \"\"\"Your job is to generate a riddle given another riddle. Only send the riddle and answer. Following is an example of a riddle you should emulate. Example: \"\"\" + getExample()\n",
        "    minimenu = \"\"\"You may 'give up' and get another question\\nYour answer: \"\"\"\n",
        "\n",
        "\n",
        "    # Generate text\n",
        "    modelOutput = promptMinstral(myInput)\n",
        "\n",
        "    # Decode and print the output\n",
        "    gameriddle, gameanswer = decodeOutput(modelOutput)\n",
        "\n",
        "    riddle_loop = True\n",
        "    while riddle_loop and self.lives > 0: # different from the gameloop, riddle loop, consists only of the sphinx's question and answering.\n",
        "      print(gameriddle)\n",
        "      if self.debug:\n",
        "        print(\"======================================================\")\n",
        "        print(\"BACKEND DETAILS: \")\n",
        "        print(modelOutput)\n",
        "        print(gameriddle)\n",
        "        print(gameanswer)\n",
        "        print(\"======================================================\")\n",
        "\n",
        "      response = input(minimenu)\n",
        "      response = response.lower()\n",
        "      responseList = response.split(\" \")\n",
        "\n",
        "\n",
        "      if gameanswer == \"None\":\n",
        "        print(\"The sphinx has a stroke and forgot to think of an answer to its own riddle. It's your lucky day\")\n",
        "        return True\n",
        "      if responseList[0] == 'give' and responseList[1] == 'up':\n",
        "        print(\"Try as you might, you can't figure this one out. The sphinx claws at you for your incompetence, costing you dearly.\")\n",
        "        self.lives -= 1\n",
        "        return False\n",
        "      elif responseList[0] == 'what':\n",
        "        print(\"The sphinx's old age is clearly getting to it. You fail to understand the question and it gets really embarassed\\nIt leaves you for now...\")\n",
        "        return True\n",
        "      if self.comp(response, gameanswer):\n",
        "        print(\"That was correct.\")\n",
        "        print(\"The sphinx looks annoyed but it sulks off in defeat.\")\n",
        "        self.location.riddle = False\n",
        "        print(\"From nowhere \" + gameanswer + \"drops to the floor from above. You can't see where it came from but it looks useful\")\n",
        "        self.location.items.append(gameanswer)\n",
        "        return True\n",
        "      else:\n",
        "        print(\"The sphinx makes a noise like laughter. You're wrong. One step closer to being sealed inside forever.\")\n",
        "        self.lives -= 1\n",
        "\n",
        "    # case where you failed the riddle and died.\n",
        "    print(\"The sphinx deems you a lost clause. The beast lunges at you and in an instant you're devoured.\")\n",
        "    return False\n",
        "\n",
        "  def prologue(self):\n",
        "    # Prologue -\n",
        "    # returns nothing but easier to navigate than multiple strings\n",
        "\n",
        "    # Updates things that should change during the game\n",
        "    bugRoom.desc = \"\"\"The sound grows louder and louder until you realize what it is. Bugs. The floor is covered in them and they all seem aggressive.\n",
        "    The room to the north is the hallway.\n",
        "    To the south seems to be your escape\"\"\"\n",
        "    start.riddle = True\n",
        "    candleRoom.riddle = True\n",
        "    escapeRoom.riddle = True\n",
        "    bugRoom.blocked = True\n",
        "    escapeRoom.blockked = True\n",
        "    candleRoom.items = [\"candle\"]\n",
        "    hallway.items =[\"rocks\"]\n",
        "\n",
        "\n",
        "    print(\"\"\"Welcome to the game. Placeholder text for the openning.\n",
        "    More story stuff to come.\n",
        "    Lot of lines\n",
        "    I kinda hope i can slow this down between each line but probably not\n",
        "    help for list of commands.\"\"\") # more to come than this but its a placeholder\n",
        "\n",
        "  def game(self):\n",
        "    # Game - Provides the game loop and main functionality of the game.\n",
        "    # The game ends when the player quits, runs out of lives or they\n",
        "    # correctly respond to three riddles.\n",
        "\n",
        "    playing = True\n",
        "\n",
        "    self.prologue()\n",
        "\n",
        "    while(playing and self.lives > 0 and not self.victory): # GAMELOOP\n",
        "\n",
        "      # Room message.\n",
        "      print(self.location.desc)\n",
        "      if(self.location.blocked):\n",
        "        print(\"You aren't be able to go any further south at this time.\")\n",
        "      if self.location.riddle:\n",
        "        print(\"The sphinx moves in front of the door. You must 'play' her game to pass.\")\n",
        "\n",
        "      # Takes user response and splits it before going through commands\n",
        "      response = input(\"\")\n",
        "      response = response.lower()\n",
        "      responseList = response.split(\" \")\n",
        "\n",
        "      # the big switch case controlling our game.\n",
        "      if(responseList[0] == \"q\" or responseList[0] == \"quit\"):\n",
        "        playing = False # User ends the game on their own, it ends here\n",
        "\n",
        "      # plays - enters the riddle.\n",
        "      elif(responseList[0] == 'play'):\n",
        "        if self.location.riddle:\n",
        "          if self.ask():\n",
        "            self.level+=1\n",
        "            print(\"Your chest swells with pride over your victory.\")\n",
        "          else:\n",
        "            print(\"You failed the sphinx's riddle!\")\n",
        "        else:\n",
        "          print(\"The sphinx is not in this room.\")\n",
        "\n",
        "      # Directional controls for the location system.\n",
        "      elif(responseList[0] == \"n\" or responseList[0] == \"north\"):\n",
        "          if self.location.north:\n",
        "            if not self.location.riddle or self.debug:\n",
        "                self.location = self.location.north\n",
        "            else:\n",
        "              print(\"The sphinx has you trapped in this room until you can answer her riddle!\")\n",
        "          else: print(\"There is no north exit.\")\n",
        "      elif(responseList[0] == \"e\" or responseList[0] == \"east\"):\n",
        "          if self.location.east:\n",
        "            if not self.location.riddle or self.debug:\n",
        "              self.location = self.location.east\n",
        "            else:\n",
        "              print(\"The sphinx has you trapped in this room until you can answer her riddle!\")\n",
        "          else: print(\"There is no east exit.\")\n",
        "      elif(responseList[0] == \"w\" or responseList[0] == \"west\"):\n",
        "          if self.location.west:\n",
        "            if not self.location.riddle or self.debug:\n",
        "              self.location = self.location.west\n",
        "            else:\n",
        "              print(\"The sphinx has you trapped in this room until you can answer her riddle!\")\n",
        "          else: print(\"There is no west exit.\")\n",
        "      elif(responseList[0] == \"s\" or responseList[0] == \"south\"):\n",
        "        if self.location.south:\n",
        "          if not self.location.blocked: # Progress will only ever go south, so we only need to check if its blocked here\n",
        "              if not self.location.riddle or self.debug: # checks if in debug mode or if you've finished the riddle\n",
        "               self.location = self.location.south\n",
        "              else:\n",
        "                print(\"The sphinx has you trapped in this room until you can answer her riddle!\")\n",
        "          else:\n",
        "            print(self.location.blocked_msg)\n",
        "        else: print(\"There is no south exit.\")\n",
        "\n",
        "      # give up command - going to phase out as we'll move the riddles to the ask function\n",
        "      elif(responseList[0] == \"give\" and responseList[1] == \"up\"):\n",
        "         print(\"You succumb to despair, losing a life\")\n",
        "         self.lives -= 1\n",
        "\n",
        "      # why - why'd you get that last answer right?\n",
        "      elif(responseList[0] == \"why\"):\n",
        "        if self.explanation:\n",
        "          print(self.explanation)\n",
        "        else:\n",
        "          print(\"Why what?\")\n",
        "\n",
        "      # look around - checks all exits\n",
        "      elif(responseList[0] == \"look\"):\n",
        "        if self.location.north:\n",
        "          print(\"There is an exit to the north\")\n",
        "        if self.location.east:\n",
        "          print(\"There is an exit to the east\")\n",
        "        if self.location.west:\n",
        "          print(\"There is an exit to the west\")\n",
        "        if self.location.south:\n",
        "          print(\"There is an exit to the south\")\n",
        "\n",
        "        print(self.location.desc)\n",
        "        if(self.location.blocked):\n",
        "          print(\"You are won't be able to go south. Something is blocking the way.\")\n",
        "        if self.location.riddle:\n",
        "          print(\"The sphinx is in this room, stopping you from leaving until you best it at its own game.\")\n",
        "        if (self.location.items): # If there are items they're listed\n",
        "          print(\"You see something useful: \")\n",
        "          for item in self.location.items:\n",
        "            print(\"\\t a \" + item)\n",
        "\n",
        "      # get - it allows players to\n",
        "      elif(responseList[0] == \"get\"):\n",
        "        if self.location.items:\n",
        "          self.inventory = self.inventory + self.location.items # conjoins the lists\n",
        "          self.location.items = [] # empties out the list of items\n",
        "        else:\n",
        "          print(\"Have you gone mad already? There's nothing there.\")\n",
        "\n",
        "      # light - our main text based puzzle - players light up their candle to unblock the bug room\n",
        "      elif(responseList[0] == \"light\" or \"fire\" in responseList):\n",
        "        if \"candle\" or \"wood\" in self.inventory:\n",
        "          if self.location == bugRoom:\n",
        "            if \"rocks\" in self.inventory:\n",
        "              print(\"You start a fire that scares the bugs away. You're now free to continue through this room\")\n",
        "              bugRoom.blocked = False\n",
        "              bugRoom.desc = \"Updated message that no longer has bugs\"\n",
        "              self.inventory.remove(\"candle\")\n",
        "              self.inventory.remove(\"rocks\")\n",
        "          else:\n",
        "            print(\"There's no reason to do that here.\")\n",
        "        else:\n",
        "          print(\"Despite your attempts to start a fire with your mind you have nothing to set ablaze\")\n",
        "      # escape - Win the game if sphinx doesn't have you in for a riddle\n",
        "      elif(responseList[0] == \"escape\"):\n",
        "        if self.location==escapeRoom and not self.location.riddle:\n",
        "          print(\"You bust through a nearby wall into the open air! You've defeated the sphinx and now you're free!\")\n",
        "          self.victory = True\n",
        "        if self.debug:\n",
        "          print(\"You win through your amazing cheating skills\")\n",
        "          self.victory = True\n",
        "\n",
        "      # help gives the controls\n",
        "      elif(responseList[0] == \"help\" or responseList[0] == \"h\"):\n",
        "        print(\"Controls :\")\n",
        "        print(\"\\t\" +\"play - In rooms where the sphinx lurks you can play its game. It won't let you go until you answer correctly.\")\n",
        "        print(\"\\t\" +\"why - after being asked a riddle and answering successfully you can hear the sphinx's explanation of the riddle.\")\n",
        "        print(\"\\t\" + \"q/quit - exits out of the game\")\n",
        "\n",
        "        print(\"\\t\" +\"Directions: \")\n",
        "        print(\"\\t\" + \"\\t\" + \"n/north - moves to the room north of this one\")\n",
        "        print(\"\\t\" + \"\\t\" + \"e/east - moves to the room east of this one\")\n",
        "        print(\"\\t\" + \"\\t\" + \"s/south - moves to the room south of this one\")\n",
        "        print(\"\\t\" + \"\\t\" + \"w/west - moves to the room west of this one\")\n",
        "\n",
        "        print(\"\\t\" +\"look - describes the room's exits, blockages and any items in it. Very useful\")\n",
        "        print(\"\\t\" +\"inventory - shows you the items you've obtained\")\n",
        "        print(\"\\t\" +\"get - adds all items in a room to your inventory.\")\n",
        "        print(\"\\t\" +\"There are more commands related to any items you might find! Try things out!\")\n",
        "\n",
        "      # else - the player misinput\n",
        "      else:\n",
        "        print(\"Not a registered command. Please type 'help' for a list of all controls.\")\n",
        "\n",
        "\n",
        "    # The gameloop has ended.\n",
        "    if self.victory:\n",
        "      print(\"Congratulations! You're free from the sphinx's game and did not meet a fate most gruesome.\")\n",
        "    else:\n",
        "      if self.lives >= 0:\n",
        "        print(\"Your adventure ends here. Your heart comes to a sudden stop.\")\n",
        "      print(\"The tomb seals shut, dooming you to spend the rest of your short life within this miserable pit.\")\n",
        "\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "playpharohs_game = Parser(True)\n",
        "pharohs_game.game()"
      ],
      "metadata": {
        "id": "zBH4sT6yVHNm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b56de8c1-fbe6-462c-bd7a-910110a3c202"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to the game. Placeholder text for the openning.\n",
            "    More story stuff to come.\n",
            "    Lot of lines\n",
            "    I kinda hope i can slow this down between each line but probably not\n",
            "    help for list of commands.\n",
            "The room you first woke up in. The air is moist and the smell of mildew fills your senses. There is an opening to the east.\n",
            "The sphinx moves in front of the door. You must 'play' her game to pass.\n",
            "play\n",
            ": How do you keep a cat from smelling?\n",
            "======================================================\n",
            "BACKEND DETAILS: \n",
            "<s>[INST] Your job is to generate a riddle given another riddle. Only send the riddle and answer. Following is an example of a riddle you should emulate. Example: Riddle: How do you keep a skunk from smelling?\n",
            "Answer: hold its nose [/INST]: How do you keep a cat from smelling?\n",
            "Answer: hold its nose</s>\n",
            ": How do you keep a cat from smelling?\n",
            "hold its nose\n",
            "======================================================\n",
            "You may 'give up' and get another question\n",
            "Your answer: hold its nose\n",
            "That was correct.\n",
            "The sphinx looks annoyed but it sulks off in defeat.\n",
            "From nowhere hold its nosedrops to the floor from above. You can't see where it came from but it looks useful\n",
            "Your chest swells with pride over your victory.\n",
            "The room you first woke up in. The air is moist and the smell of mildew fills your senses. There is an opening to the east.\n",
            "e\n",
            "A room that connects to three others.\n",
            "It's dim but you can faintly see a light from the room to the west.\n",
            "The room to the south is loud. It sounds like something is alive down there.\n",
            "The room to the east is where you woke up.\n",
            "e\n",
            "A surprisingly bright room for the tomb. Its lined with many fires, and the walls seem gilded with gold. You're unfortunately unable to collect any.\n",
            "The room to the west is the hallway.\n",
            "The sphinx moves in front of the door. You must 'play' her game to pass.\n",
            "play\n",
            ":Answer: get dressed\n",
            "======================================================\n",
            "BACKEND DETAILS: \n",
            "<s>[INST] Your job is to generate a riddle given another riddle. Only send the riddle and answer. Following is an example of a riddle you should emulate. Example: Riddle: What did the doctor say when the patient said''Doctor Doctor i feel like cvurtains''??\n",
            "Answer: then pull yourself together [/INST]:Answer: get dressed</s>\n",
            ":Answer: get dressed\n",
            "get dressed\n",
            "======================================================\n",
            "You may 'give up' and get another question\n",
            "Your answer: give up\n",
            "Try as you might, you can't figure this one out. The sphinx claws at you for your incompetence, costing you dearly.\n",
            "You failed the sphinx's riddle!\n",
            "A surprisingly bright room for the tomb. Its lined with many fires, and the walls seem gilded with gold. You're unfortunately unable to collect any.\n",
            "The room to the west is the hallway.\n",
            "The sphinx moves in front of the door. You must 'play' her game to pass.\n",
            "play\n",
            "Answer: jesus\n",
            "======================================================\n",
            "BACKEND DETAILS: \n",
            "<s>[INST] Your job is to generate a riddle given another riddle. Only send the riddle and answer. Following is an example of a riddle you should emulate. Example: Riddle: What is \"j\" in \"wwjd\"?\n",
            "Answer: jesus [/INST]Answer: jesus</s>\n",
            "Answer: jesus\n",
            "jesus\n",
            "======================================================\n",
            "You may 'give up' and get another question\n",
            "Your answer: jesus\n",
            "That was correct.\n",
            "The sphinx looks annoyed but it sulks off in defeat.\n",
            "From nowhere jesusdrops to the floor from above. You can't see where it came from but it looks useful\n",
            "Your chest swells with pride over your victory.\n",
            "A surprisingly bright room for the tomb. Its lined with many fires, and the walls seem gilded with gold. You're unfortunately unable to collect any.\n",
            "The room to the west is the hallway.\n",
            "get\n",
            "A surprisingly bright room for the tomb. Its lined with many fires, and the walls seem gilded with gold. You're unfortunately unable to collect any.\n",
            "The room to the west is the hallway.\n",
            "exit\n",
            "Not a registered command. Please type 'help' for a list of all controls.\n",
            "A surprisingly bright room for the tomb. Its lined with many fires, and the walls seem gilded with gold. You're unfortunately unable to collect any.\n",
            "The room to the west is the hallway.\n",
            "quit\n",
            "Your adventure ends here. Your heart comes to a sudden stop.\n",
            "The tomb seals shut, dooming you to spend the rest of your short life within this miserable pit.\n"
          ]
        }
      ]
    }
  ]
}